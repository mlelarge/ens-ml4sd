{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find an online version in Google colab here: https://colab.research.google.com/drive/1DfxpKnoAnSePEWHWssp29zraRlpH40ZM?usp=sharing.\n",
    "\n",
    "# 1. Refresher about unsupervised learning\n",
    "\n",
    "**Unsupervised Learning** aims at finding structures inside a dataset. Usually, it implies to model the underlying probability distribution of the data $\\rho(x)$. \n",
    "\n",
    "Examples of tasks in this category are:\n",
    "- Clustering: group samples based on some similarity measure,\n",
    "- Dimensionality reduction: Find a low-dimensional representation of high-dimensional data,\n",
    "- Generative modelling: Generate samples from the unkown law $\\rho(x)$,\n",
    "- Also: Density estimation, representation learning, sampling, etc.\n",
    "\n",
    "Compared to supervised learning, unsupervised learning has does not exploit **labels**, the dataset consists in $n$ **training samples** $\\{x_i\\}_{i=1}^n$, where $x_i \\in \\mathbb{R}^d$ (think of a feature vector, an image, a sound, etc.).\n",
    "\n",
    "In this notebook, we are focusing on one of the simplest algorithm to perform **clustering**: the K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-means principle\n",
    "\n",
    "In K-means, we assume the dataset is split into $K$ clusters, each having a center $\\mu_k$ to which are assigned the datapoints falling the closest to $\\mu_k$ (in Euclidean distance). Mathematically, the K-means cost reads\n",
    "$$\n",
    "R(X, \\bm{C}, \\mu) = \\sum_{k=1}^K \\sum_{i\\in C_k} \\lVert x_i - \\mu_k \\rVert_2^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mu_k = \\frac{1}{\\lvert C_k\\rvert}\\sum_{i\\in C_k} x_i.\n",
    "$$\n",
    "\n",
    "Minimizing the cost function jointly over the cluster centers $\\mu_k$ for each $k$ and the assignments $\\bm{C}$ is a hard nonconvex and combinatorial task. But remark that it is in fact easy to solve if we have access to a given clustering $\\bm{C}$, meaning we know which $x_i$ belongs to class $k$. In this case, we just have to compute the barycenter of each cluster to get the centers $\\{\\mu_k\\}_{k=1}^K$. Alternatively, if we have center of clusters, then the assignment is trivially given by nearest center to a training point.\n",
    "\n",
    "With this simple reasoning, we drew the **natural K-Means algorithm** that we will implement which reads:\n",
    "1. Initialize centers $\\mu_1, \\ldots, \\mu_K$ randomly in $\\mathbb{R}^d$,\n",
    "2. Repeat until convergence:\n",
    "    1. Compute, for each training point $x_i$, the *assignment* $a_i = \\text{argmin}_k \\lVert x_i - \\mu_k \\rVert_2^2$,\n",
    "    2. Compute, for each cluster $k$, the *center* $\\mu_k = \\frac{1}{\\lvert C_k\\rvert}\\sum_{i\\in C_k} x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_zQLOkuQYYJ"
   },
   "source": [
    "The cell below just loads some packages we will need to implement the algorithm and visuals the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LseJTDjOC7Mf"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons           # Contains some dataset we will be using\n",
    "from sklearn.cluster import KMeans                # Contains an implementation of the K-Means\n",
    "from sklearn.preprocessing import StandardScaler  # Allows to standardize the features\n",
    "import matplotlib.pyplot as plt                   # Used for plotting\n",
    "import numpy as np                                # Used for handling linear algebra on vectors/arrays\n",
    "from PIL import Image                             # Used to load JPEG images\n",
    "from tqdm import tqdm                             # Used to display progress bars \n",
    "import matplotlib as mpl\n",
    "\n",
    "# Formatting the plots\n",
    "plt.rcParams['figure.figsize'] = [6,6]\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.style.use('default')\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['font.size'] = 22\n",
    "mpl.rcParams['axes.formatter.limits'] = (-6, 6)\n",
    "mpl.rcParams['axes.formatter.use_mathtext'] = True\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
    "mpl.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
    "mpl.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
    "mpl.rcParams['xtick.minor.visible'] = True\n",
    "mpl.rcParams['ytick.minor.visible'] = True\n",
    "\n",
    "!git clone https://github.com/mlelarge/ens-ml4sd.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data: a simple clustering problem\n",
    "\n",
    "To understand and implement the algorithm, let us consider a synthetic dataset made of $n=300$ datapoints coming from three well-separated two-dimensional Gaussian clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "l1 = [-3, 0]; l2 = [0, 4]; l3 = [3, -1]\n",
    "A = rng.normal(loc=l1, scale=0.6, size=(100,2))\n",
    "B = rng.normal(loc=l2, scale=0.5, size=(100,2))\n",
    "C = rng.normal(loc=l3, scale=0.7, size=(100,2))\n",
    "X = np.vstack([A,B,C])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plt.plot(X[:,0], X[:,1], 'k.', markersize=8)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q1.** With this simple example, we have the chance to fully visualize the dataset. What number of clusters $K$ would you choose to run the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This choice is not trivial in practical applications as the datasets are often both high-dimensional and not well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4ltYIsaDJGP"
   },
   "source": [
    "# 4. Implementing K-means in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the algorithm, we will follow a matrix convention where we store all the training points in line and each column correspond to a feature. Mathematically, we call $X \\in \\mathbb{R}^{n\\times d}$ the feature matrix. For centers, we will also write $M \\in \\mathbb{R}^{K\\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below is a function allowing to compute the distance between each $x_i$ and each $\\mu_k$. It returns a squared distance matrix $D \\in \\mathbb{R}^{n\\times K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(X, centers):\n",
    "    \"\"\"\n",
    "    Compute squared Euclidean distances between each row of X and each center.\n",
    "    X : (n, d) numpy array\n",
    "    centers : (K, d) numpy array\n",
    "    returns d2 : (n, K) numpy array of distances between x_i and center_j\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    K = centers.shape[0]\n",
    "    d2 = np.zeros((n, K), dtype=float)\n",
    "    for j in range(K):      # For each cluster j\n",
    "        diff = X - centers[j]                   # Shape (n, d) : subtract center j from all points\n",
    "        d2[:, j] = np.sum(diff**2, axis=1)      # Squared Euclidean distance\n",
    "    return d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better-use of the power of the `numpy` library is to use **broadcasting** to compute the distances. Broadcasting is a way to perform elementwise operations on arrays of different shapes without having to replicate the matrices or a `for` loop. Here is a simple example of broadcasting in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])   # shape (2,3)\n",
    "\n",
    "b = np.array([10, 20, 30])  # shape (3,)\n",
    "\n",
    "C = A + b   # shape (2,3), b is broadcast across rows\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark how $b$ is added to each row of $A$ without explicitly mentionning it. Below we rewrite the `compute_distances`function using this trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(X, centers):\n",
    "    \"\"\"\n",
    "    Compute squared Euclidean distances between each row of X and each center using broadcasting.\n",
    "    X : (n, d) numpy array\n",
    "    centers : (K, d) numpy array\n",
    "    returns d2 : (n, K) numpy array of distances between x_i and center_j\n",
    "    \"\"\"\n",
    "    # Use broadcasting to compute all distances at once\n",
    "    diff = X[:, np.newaxis, :] - centers[np.newaxis, :, :]  # Shape: (n, K, d)\n",
    "    return np.sum(diff**2, axis=2)  # Shape: (n, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q2.** Based on this distance function, complete the following code to assign the different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(X, centers):\n",
    "    '''\n",
    "    X : (n, d) numpy array\n",
    "    centers : (K, d) numpy array\n",
    "    returns assignments : (n,) numpy array of cluster indices\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q3.** Similarly, complete the following code to compute the new centers given a set of estimated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "W-Aii9b2Lx2N",
    "outputId": "da8ae4f4-5995-43eb-fe68-94ce93334055"
   },
   "outputs": [],
   "source": [
    "def compute_centers(X, labels, K):\n",
    "    '''\n",
    "    X : (n, d) numpy array\n",
    "    labels : (n,) numpy array of cluster indices\n",
    "    K : number of clusters\n",
    "    returns centers : (K, d) numpy array of new center positions\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q4.** Now complete the function to compute the cost $R(X, M)$ of a given assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, centers, labels):\n",
    "    '''\n",
    "    X : (n, d) numpy array\n",
    "    centers : (K, d) numpy array\n",
    "    labels : (n,) numpy array of cluster indices\n",
    "    returns cost : float, the K-Means cost function evaluated on X with given centers and labels\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to put it all together.\n",
    "\n",
    "> **Q5.** Complete the function below to implement the K-Means algorithm using our previous codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_numpy(X, K, N_iters=100, seed=None):\n",
    "    \"\"\"\n",
    "    X : (n, d) numpy array of data points\n",
    "    K : number of clusters\n",
    "    N_iters : maximum number of iterations\n",
    "    seed : random seed for reproducibility\n",
    "    returns centers_history : (N_iters+1, K, d) numpy array of centers over iterations\n",
    "            labels : (n,) numpy array of final cluster assignments\n",
    "            cost_history : list of costs over iterations\n",
    "            final_cost : final cost value\n",
    "    \"\"\"\n",
    "    # --- Fixing the seed and preparing variables ---\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n, d = X.shape\n",
    "\n",
    "    # --- Initialization (random training points) ---\n",
    "    initial_idx = rng.choice(n, size=K, replace=False)\n",
    "    centers = X[initial_idx].copy()\n",
    "\n",
    "    # --- Storing vectors ---\n",
    "    centers_history = [centers.copy()]\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    cost_history = []\n",
    "\n",
    "    # --- Training loop ---\n",
    "    for it in tqdm(range(N_iters)):       # Loop for a maximum of N_iters iterations\n",
    "        # -----------------------\n",
    "        # ----> YOUR CODE HERE\n",
    "        # -----------------------\n",
    "        \n",
    "        # HINTS:\n",
    "        # 1) Compute the assignments and cost\n",
    "        # 2) Compute the new centers\n",
    "        # 3) Update for the next iteration\n",
    "        \n",
    "        # Store history\n",
    "        cost_history.append(cost)                               # Store the cost\n",
    "        centers_history.append(new_centers.copy())              # Store the center positions\n",
    "    \n",
    "    return np.array(centers_history), labels, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q6.** It's time to train our algorithm! Call the `kmeans_numpy` with a value of $K=3$, for 10 iterations and with `seed=15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q7.** Below we plot the several estimated centers during training. Vary $K$ and the seed for intialization. Comment your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the centers together with the training points\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=labels, cmap='tab10', s=18, alpha=0.7)\n",
    "cmap = plt.get_cmap('tab10')\n",
    "for j in range(K):\n",
    "    traj = centers_history[:, j, :]\n",
    "    plt.plot(traj[:,0], traj[:,1], marker='o', linestyle='-', color=cmap(j))\n",
    "    plt.scatter(traj[0,0], traj[0,1], marker='s', s=90, edgecolor='k', facecolor=cmap(j), zorder=5)\n",
    "    plt.scatter(traj[-1,0], traj[-1,1], marker='X', s=120, edgecolor='k', facecolor=cmap(j), zorder=6)\n",
    "plt.title(\"Centers trajectories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost function\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cost_history, marker='o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost function over iterations\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX4yM_PhNBC9"
   },
   "source": [
    "# 5. Scikit-learn's K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, everything we implemented is done in a well-known machine learning library called [`scikit-learn`](https://scikit-learn.org/stable/modules/clustering.html#clustering). It implements much more than the K-Means algorithm as shown in the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q8.** Use the `KMeans` class from `sklearn.cluster` to fit the same dataset with the same $K$ and number of iterations. Compare the results with your implementation. You can access the cluster centers with the attribute `cluster_centers_` and the labels with `labels_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE for scikit-learn comparison\n",
    "\n",
    "\n",
    "\n",
    "labels_sklearn = \n",
    "centers_sklearn =\n",
    "\n",
    "\n",
    "# ---------------\n",
    "# Plot function\n",
    "# ---------------\n",
    "def plot_kmeans(X, labels, centers, ax):\n",
    "    ax.scatter(X[:,0], X[:,1], c=labels, cmap='tab10', s=18, alpha=0.7)\n",
    "    ax.scatter(centers[:,0], centers[:,1], marker='X', s=120, edgecolor='k', facecolor='r', zorder=6)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "plot_kmeans(X, labels_sklearn, centers_sklearn, ax[0])\n",
    "ax[0].set_title(\"Scikit-learn\")\n",
    "\n",
    "plot_kmeans(X, labels, centers_history[-1], ax[1])\n",
    "ax[1].set_title(\"Ours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q9.** Vary the seeds in the code above. Do you observe the same problem in the scikit-learn method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu-NQ-aGDBtZ"
   },
   "source": [
    "\n",
    "# 6. A case where K-means fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjGNBh78NWro"
   },
   "source": [
    "> **Q10.** In which case(s) do you think K-means would fail to find proper clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following dataset made of two moons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "H4K-8TTf8Laz",
    "outputId": "c12c27ee-8d74-4b3f-f780-b926c3ac49a3"
   },
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.07, random_state=42)\n",
    "X_moons = StandardScaler().fit_transform(X_moons)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"True labels\")\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], c=y_moons, cmap='tab10', s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLZmlhlxCkfT"
   },
   "source": [
    "> **Q11.** Apply the K-means algorithm to these data, show the clustering, and conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfnCgDqtCkMq"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72wCK1uuBqWZ"
   },
   "source": [
    "# 7. Image quantization\n",
    "\n",
    "Image quantization is a task of reducing the number of distinct colours used to represent an image. Formally, we want to map each pixel value to a finite palette $\\mathcal{P}={p_1, \\ldots, p_K}_{k=1}^K$. The quantization look to minimize the distortion between the original and quantized image. We can view K-Means as a quantization algorithm where the number of clusters $K$ defines the size of the palette and the $p_k$ corresponds to the centers $\\mu_k$ defined above.\n",
    "\n",
    "> **Q12.** Vary the number of clusters $K$ in the code below to observe how the distortions evolve. Comment the case $K=2$: to what correspond the two classes visually, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "CIaYqURVAkYN",
    "outputId": "d28f9872-04a3-4d1b-d06a-0c6f80a55c94"
   },
   "outputs": [],
   "source": [
    "# Load the image in colab and open it\n",
    "img = Image.open('./ens-ml4sd/notebooks/data/NGC4414.jpg').convert('RGB')\n",
    "img = img.resize((256, 256))          # Resize the image to speed up\n",
    "arr = np.array(img) / 255.0           # float [0,1], shape (H,W,3)\n",
    "H, W, C = arr.shape\n",
    "pixels = arr.reshape(-1, 3)\n",
    "\n",
    "K = 2       # Number of K-means clusters\n",
    "\n",
    "km = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "labels = km.fit_predict(pixels)\n",
    "centers = km.cluster_centers_\n",
    "\n",
    "# reconstruct compressed image\n",
    "quant = centers[labels].reshape(H, W, 3)\n",
    "\n",
    "# Plot the original image and the quantized one\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1); plt.title('Original'); plt.imshow(arr); plt.axis('off')\n",
    "plt.subplot(1,2,2); plt.title(r'Quantization ($K=${:d})'.format(K)); plt.imshow(quant); plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
